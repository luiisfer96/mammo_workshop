{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4006e83e",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"text-align: center;\">End-to-End Mammography Inference Workshop</h1>\n",
    "\n",
    "<p>\n",
    "  <b>Based on:</b> Shen et al., \n",
    "  <i>Scientific Reports</i> (2019) — \n",
    "  \"Deep Learning to Improve Breast Cancer Detection on Screening Mammography\".<br>\n",
    "  <b>Code base:</b> \n",
    "  <a href=\"https://github.com/lishen/end2end-all-conv\" target=\"_blank\">\n",
    "    lishen/end2end-all-conv\n",
    "  </a> <br>\n",
    "  <b>Models:</b> \n",
    "  <a href=\"https://drive.google.com/drive/folders/0B1PVLadG_dCKV2pZem5MTjc1cHc?resourcekey=0-t4vtopuv27D9NnMC97w6hg\" target=\"_blank\">\n",
    "    Folder Models\n",
    "  </a>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  This notebook covers: environment setup, preprocessing, inference with pretrained models, \n",
    "  and evaluation. It also includes a short theoretical introduction to classifiers, \n",
    "  datasets, and architectures.\n",
    "</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69336ff0",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Breast cancer remains one of the leading causes of mortality among women worldwide, which has driven the development of **artificial intelligence–based diagnostic support tools**. In particular, **deep learning methods applied to mammography** have shown promising results in improving the sensitivity and specificity of computer-aided detection (CAD) systems.  \n",
    "\n",
    "One of the most relevant contributions in this area was made by **Shen et al. (2019)**, who proposed an *end-to-end* training pipeline. Their approach combines patch-level classifiers with whole-image classifiers, significantly reducing the dependency on detailed region of interest (ROI) annotations. This strategy enables models to be trained with a combination of richly annotated datasets and larger collections labeled only at the image level.  \n",
    "\n",
    "---\n",
    "\n",
    "<div align=\"center\">\n",
    "<h3>END TO END ARCHITECTURE<h3>\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "  <img src=\"Assets/end2end-architecture.png\" alt=\"End-to-End Architecture\" width=\"650\"/>\n",
    "  <img src=\"Assets/yaroslav-architecture.png\" alt=\"Yaroslav Architecture\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c416034e",
   "metadata": {},
   "source": [
    "After defining the model architecture, Shen et al. validated their method on two widely used public datasets:  \n",
    "\n",
    "- **CBIS-DDSM**: a curated subset of the DDSM database, with 2,478 digitized mammograms and lesion-level annotations, used for the initial training of patch classifiers.  \n",
    "- **INbreast**: a dataset with 410 full-field digital mammography (FFDM) images, mainly used to evaluate model transferability and generalization to higher-quality clinical images【Shen et al., 2019】.  \n",
    "\n",
    "### Datasets Samples and URL to download it.\n",
    "\n",
    "<div align=\"center\" style=\"display: flex; justify-content: center; gap: 40px;\">\n",
    "  <figure style=\"text-align: center;\">\n",
    "    <img src=\"Assets/cbis-example.jpg\" width=\"300\" height=\"420\"/>\n",
    "    <figcaption>\n",
    "      <a href=\"https://www.kaggle.com/datasets/awsaf49/cbis-ddsm-breast-cancer-image-dataset\" target=\"_blank\">\n",
    "        CBIS-DDSM Breast Cancer Image Dataset\n",
    "      </a>\n",
    "    </figcaption>\n",
    "  </figure>\n",
    "  \n",
    "  <figure style=\"text-align: center;\">\n",
    "    <img src=\"Assets/inbreast-example.png\" width=\"390\" height=\"400\"/>\n",
    "    <figcaption>\n",
    "      <a href=\"https://www.kaggle.com/datasets/ramanathansp20/inbreast-dataset\" target=\"_blank\">\n",
    "        INBreast Dataset\n",
    "      </a>\n",
    "    </figcaption>\n",
    "  </figure>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1199bd2",
   "metadata": {},
   "source": [
    "In this notebook, we document the process of **implementing, adapting, and evaluating these models using an alternative dataset**. Specifically, we use images acquired with **MicroDose SI (Philips Healthcare, The Netherlands)** and **Senographe Essential (GE Healthcare, USA)** systems, managed through **Optomed software (Finland)**. This dataset includes **1,528 images from 382 women**, each with the two standard views: **cranio-caudal (CC)** and **medio-lateral oblique (MLO)**. All images were standardized to a resolution of **100 µm/pixel** and stored in 16-bit format【Scientific Reports, 2023】.  \n",
    "\n",
    "<div align=\"center\" style=\"display: flex; justify-content: center; gap: 20px; align-items: flex-start;\">\n",
    "  <figure style=\"text-align: center; width: 350px; margin: 0;\">\n",
    "    <img src=\"Assets/controls-example.png\" width=\"350\" height=\"400\"/>\n",
    "    <figcaption><b>Controls</b></figcaption>\n",
    "  </figure>\n",
    "\n",
    "  <figure style=\"text-align: center; width: 350px; margin: 0;\">\n",
    "    <img src=\"Assets/cases-example.png\" width=\"350\" height=\"400\"/>\n",
    "    <figcaption><b>Cases</b></figcaption>\n",
    "  </figure>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\" style=\"margin-top: 5px; color: red; font-weight: bold;\">\n",
    "  ⚠️ Please note: This dataset will be used for inference. Make sure to download it and place it in the root directory of the project. To get de dataset ask to the autors or find it the CPS DC channel  \n",
    "</div>\n",
    "\n",
    "The main objective is to **adapt and preprocess this dataset so that the predictions of the pre-trained models are consistent with the available clinical annotations**, while assessing network performance in a scenario different from the one originally proposed. This will help evaluate the **transferability and robustness of the models** for use with heterogeneous datasets, a key requirement for real-world clinical applications.  \n",
    "\n",
    "---\n",
    "\n",
    "**References:**  \n",
    "- Shen L, Margolies LR, Rothstein JH, Fluder E, McBride R, Sieh W. *Deep Learning to Improve Breast Cancer Detection on Screening Mammography*. Scientific Reports. 2019; 9:12495. https://doi.org/10.1038/s41598-019-48995-4  \n",
    "- Scientific Reports. *Automated breast cancer detection in digital mammograms using deep learning*. 2023. https://www.nature.com/articles/s41598-023-46921-3\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5c5f89",
   "metadata": {},
   "source": [
    "## 1) Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42532e69",
   "metadata": {},
   "source": [
    "<h4>A) Install</h4>\n",
    "\n",
    "<p>\n",
    "  Before creating the working environment and installing the required libraries, \n",
    "  it is essential to install three fundamental programs on your system:\n",
    "</p>\n",
    "<ul>\n",
    "  <li><b>Python 3.7.0:</b> the specific version of the language required to ensure compatibility with the libraries used.</li>\n",
    "  <li><b>Anaconda:</b> simplifies the management of virtual environments and dependencies, making sure each project is isolated and well organized.</li>\n",
    "  <li><b>ImageMagick:</b> an image processing tool necessary for the correct execution of some project functions. <strong>In windows it's necessary to install the .dll</strong> in Ubuntu just the library with pip.</li>\n",
    "</ul>\n",
    "<p>\n",
    "  These programs must be installed in the order shown, as they form the foundation \n",
    "  on which the development environment will be configured and the required libraries \n",
    "  will later be installed.\n",
    "</p>\n",
    "\n",
    "\n",
    "<div align=\"center\" style=\"display: flex; justify-content: center; gap: 20px; align-items: flex-start;\">\n",
    "  <figure style=\"text-align: center; width: 300px; margin: 0;\">\n",
    "    <img src=\"Assets/python-install.png\" width=\"300\" height=\"200\"/>\n",
    "    <figcaption>\n",
    "      <b>Python 3.7.0</b><br>\n",
    "      <a href=\"https://www.python.org/downloads/release/python-370/\" target=\"_blank\">\n",
    "        Download Link\n",
    "      </a>\n",
    "    </figcaption>\n",
    "  </figure>\n",
    "\n",
    "  <figure style=\"text-align: center; width: 300px; margin: 0;\">\n",
    "    <img src=\"Assets/anaconda-install.png\" width=\"300\" height=\"200\"/>\n",
    "    <figcaption>\n",
    "      <b>Anaconda</b><br>\n",
    "      <a href=\"https://www.anaconda.com/download\" target=\"_blank\">\n",
    "        Download Link\n",
    "      </a>\n",
    "    </figcaption>\n",
    "  </figure>\n",
    "\n",
    "  <figure style=\"text-align: center; width: 300px; margin: 0;\">\n",
    "    <img src=\"Assets/imagemagick-install.png\" width=\"300\" height=\"200\"/>\n",
    "    <figcaption>\n",
    "      <b>ImageMagick</b><br>\n",
    "      <a href=\"https://imagemagick.org/archive/binaries/ImageMagick-7.1.2-2-Q16-x64-dll.exe\" target=\"_blank\">\n",
    "        Download Link\n",
    "      </a>\n",
    "    </figcaption>\n",
    "  </figure>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc065fe",
   "metadata": {},
   "source": [
    "---\n",
    "<h4>C) Create environtment with conda</h4>\n",
    "\n",
    "Open an Anaconda prompt and follow this commands below:\n",
    "\n",
    "<pre><code>\n",
    "# 1. Create environment in the file folder\n",
    "conda create -n end2end python=3.7.0\n",
    "</code></pre>\n",
    "\n",
    "<pre><code>\n",
    "# 2. Activate conda\n",
    "conda activate end2end\n",
    "</code></pre>\n",
    "\n",
    "<pre><code>\n",
    "# 3. Upgrade pip\n",
    "python -m pip install --upgrade pip\n",
    "</code></pre>\n",
    "\n",
    "<pre><code>\n",
    "# 4. Install ipykernel\n",
    "python -m pip install ipykernel\n",
    "</code></pre>\n",
    "\n",
    "<p style=\"text-align: justify;\">\n",
    "  Creating a dedicated environment with <b>conda</b> ensures that all dependencies \n",
    "  are isolated from the global system, preventing version conflicts between \n",
    "  different projects. After creating and activating the environment, \n",
    "  <b>pip</b> is upgraded to guarantee compatibility with the latest package manager \n",
    "  features. Finally, the installation of <b>ipykernel</b> is essential because it \n",
    "  allows Jupyter Notebooks to recognize and use this environment as a \n",
    "  <i>kernel</i>. Without ipykernel, the environment would exist, but it would not \n",
    "  appear in the list of available kernels in VS Code or Jupyter, making it \n",
    "  impossible to run notebook cells inside that specific environment.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d232a3",
   "metadata": {},
   "source": [
    "<h4>D) Select the Kernel in VS Code</h4>\n",
    "\n",
    "<p style=\"text-align: justify;\">\n",
    "  Before running any cell in a Jupyter Notebook, it is important to first \n",
    "  <b>install the Jupyter extension in VS Code</b>.  \n",
    "  Once installed, you can <b>select the correct Kernel</b> by clicking on the \n",
    "  kernel icon located at the <b>top right</b> of VS Code and choosing the \n",
    "  appropriate environment from the list..\n",
    "</p>\n",
    "\n",
    "<div align=\"center\" style=\"margin: 20px 0; white-space: nowrap;\">\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 0 15px;\">\n",
    "    <img src=\"Assets/jupyter-extension.png\" alt=\"Jupyter Extension Installation\" width=\"300\"/>\n",
    "    <figcaption><i>1. Jupyter extension installation</i></figcaption>\n",
    "  </figure>\n",
    "\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 0 15px;\">\n",
    "    <img src=\"Assets/icone.png\" alt=\"Kernel Icon\" width=\"180\"/>\n",
    "    <figcaption><i>2. Kernel icon</i></figcaption>\n",
    "  </figure>\n",
    "\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 0 15px;\">\n",
    "    <img src=\"Assets/kernel-end2end.png\" alt=\"Kernel Selection Menu\" width=\"400\"/>\n",
    "    <figcaption><i>3. Kernel selection menu</i></figcaption>\n",
    "  </figure>\n",
    "</div>\n",
    "\n",
    "<p style=\"text-align: justify; margin-top: 20px;\">\n",
    "  Now, you are ready to start running cells in your Jupyter Notebook.  \n",
    "  As a first test, you can try running the following code:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45e9e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luiis\\.conda\\envs\\end2end\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75193e7",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "  This command prints the full path of the Python interpreter currently being used \n",
    "  by the selected Kernel. It is very useful to verify that your notebook is \n",
    "  running on the correct virtual environment or interpreter you intended to use.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align: justify;\">\n",
    "Before running the experiments, execute the verification block below to ensure that all required datasets \n",
    "(<code>nyu_controls.pkl</code> and <code>nyu_gmic.pkl</code>) and project directories are correctly placed in the root folder. \n",
    "This step prevents errors related to missing files or incorrect folder structure, ensuring full reproducibility of the results.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d2c21d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found: nyu_controls.pkl\n",
      "✅ Found: nyu_gmic.pkl\n",
      "✅ Found directory: Articles\n",
      "✅ Found directory: Assets\n",
      "✅ Found directory: Installers\n",
      "✅ Found directory: Models\n",
      "✅ Found directory: RESULTS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "required_files = [\"nyu_controls.pkl\", \"nyu_gmic.pkl\"]\n",
    "required_dirs = [\"Articles\", \"Assets\", \"Installers\", \"Models\", \"RESULTS\"]\n",
    "\n",
    "# Check files\n",
    "for f in required_files:\n",
    "    if not os.path.isfile(f):\n",
    "        print(f\"❌ Missing file: {f}\")\n",
    "    else:\n",
    "        print(f\"✅ Found: {f}\")\n",
    "\n",
    "# Check directories\n",
    "for d in required_dirs:\n",
    "    if not os.path.isdir(d):\n",
    "        print(f\"❌ Missing directory: {d}\")\n",
    "    else:\n",
    "        print(f\"✅ Found directory: {d}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e40f97",
   "metadata": {},
   "source": [
    "<h4 style=\"margin-top: 30px;\">D) Install requirements manually in the conda prompt using pip</h4>\n",
    "\n",
    "<p style=\"text-align: justify;\">\n",
    "  At this stage, it is necessary to install all the required libraries listed in \n",
    "  the <code>requirements.txt</code> file.  \n",
    "  However, based on experience, it is often more reliable to install the packages \n",
    "  one by one rather than all at once.  \n",
    "  When trying to install them using the following command in a notebook cell, the \n",
    "  process failed:\n",
    "</p>\n",
    "\n",
    "<pre>\n",
    "!pip install -r requirements.txt\n",
    "</pre>\n",
    "\n",
    "<p style=\"text-align: justify;\">\n",
    "  To avoid dependency conflicts or installation errors, it is recommended to \n",
    "  install the libraries individually using <code>pip</code> directly in the \n",
    "  conda prompt.  \n",
    "  Below are the installation commands for each required package:\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "  <li><code>pip install tensorflow==2.8.1</code></li>\n",
    "  <li><code>pip install protobuf==3.20.1</code></li>\n",
    "  <li><code>pip install opencv-python</code></li>\n",
    "  <li><code>pip install wand==0.6.11</code></li>\n",
    "  <li><code>pip install matplotlib</code></li>\n",
    "  <li><code>pip install pandas</code></li>\n",
    "  <li><code>pip install scikit-learn</code></li>\n",
    "  <li><code>pip install tqdm</code></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14efa83",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "  Once all dependencies are installed, you can verify the installation by running \n",
    "  the following command in the conda prompt:\n",
    "</p>\n",
    "\n",
    "<pre>\n",
    "pip list\n",
    "</pre>\n",
    "\n",
    "<div align=\"center\" style=\"display: flex; justify-content: center; align-items: flex-start;\">\n",
    "  <figure style=\"text-align: center; width: 400px;\">\n",
    "    <img src=\"Assets/conda-prompt.png\" width=\"400\" height=\"400\"/>\n",
    "  </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67788150",
   "metadata": {},
   "source": [
    "## 2) Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2f0dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from pprint import pprint\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341ba102",
   "metadata": {},
   "source": [
    "## 3) Metadata (.pkl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89316ca5",
   "metadata": {},
   "source": [
    "####  Case & Control Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36892457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Cargando cases.pkl...\n",
      "   Total de registros en cases.pkl: 191\n",
      "\n",
      "🔹 cases.pkl - Registro aleatorio 1:\n",
      "{'L-CC': ['66ca_100_L_CC'],\n",
      " 'L-MLO': ['66ca_100_L_MLO'],\n",
      " 'R-CC': ['66ca_100_R_CC'],\n",
      " 'R-MLO': ['66ca_100_R_MLO'],\n",
      " 'best_center': {'L-CC': [(1531, 1432)], 'L-MLO': [(1588, 1432)], 'R-CC': [(1531, 1433)], 'R-MLO': [(1588, 1433)]},\n",
      " 'bottommost_points': {'L-CC': [(3061, (100, 2293))],\n",
      "                       'L-MLO': [(3061, (100, 2293))],\n",
      "                       'R-CC': [(3061, (101, 2294))],\n",
      "                       'R-MLO': [(3061, (101, 2294))]},\n",
      " 'cancer_label': {'left_benign': 0, 'left_malignant': 1, 'right_benign': 0, 'right_malignant': 0},\n",
      " 'distance_from_starting_side': {'L-CC': [0], 'L-MLO': [0], 'R-CC': [0], 'R-MLO': [0]},\n",
      " 'horizontal_flip': 'NO',\n",
      " 'rightmost_points': {'L-CC': [((1020, 2961), 2393)],\n",
      "                      'L-MLO': [((1020, 2961), 2393)],\n",
      "                      'R-CC': [((1020, 2961), 2394)],\n",
      "                      'R-MLO': [((1020, 2961), 2394)]},\n",
      " 'window_location': {'L-CC': [(0, 3062, 0, 2394)],\n",
      "                     'L-MLO': [(0, 3062, 0, 2394)],\n",
      "                     'R-CC': [(0, 3062, 0, 2394)],\n",
      "                     'R-MLO': [(0, 3062, 0, 2394)]}}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Cargando controls.pkl...\n",
      "   Total de registros en controls.pkl: 191\n",
      "\n",
      "🔹 controls.pkl - Registro aleatorio 1:\n",
      "{'L-CC': ['13co_13_L_CC'],\n",
      " 'L-MLO': ['13co_13_L_MLO'],\n",
      " 'R-CC': ['13co_13_R_CC'],\n",
      " 'R-MLO': ['13co_13_R_MLO'],\n",
      " 'cancer_label': {'left_benign': 0, 'left_malignant': 0, 'right_benign': 0, 'right_malignant': 0}}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Definir rutas de los PKL\n",
    "pkl_files = {\n",
    "    \"cases.pkl\": r\"nyu_gmic.pkl\",\n",
    "    \"controls.pkl\": r\"nyu_controls.pkl\"\n",
    "}\n",
    "\n",
    "for nombre, ruta in pkl_files.items():\n",
    "    print(f\"\\n Cargando {nombre}...\")\n",
    "    with open(ruta, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    print(f\"   Total de registros en {nombre}: {len(data)}\\n\")\n",
    "\n",
    "    # Seleccionar 2 registros aleatorios\n",
    "    muestras = random.sample(data, 1)\n",
    "\n",
    "    for i, registro in enumerate(muestras, 1):\n",
    "        print(f\"🔹 {nombre} - Registro aleatorio {i}:\")\n",
    "        pprint(registro, width=120)\n",
    "        print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290fd757",
   "metadata": {},
   "source": [
    "<p>\n",
    "In this step, we load two <code>.pkl</code> files (<code>cases.pkl</code> and <code>controls.pkl</code>) that contain structured information about mammography exams. These files are essential for the <b>preprocessing and inference stages</b>, since they store metadata about each case and control subject.\n",
    "</p>\n",
    "\n",
    "<p><b>The script performs the following tasks:</b></p>\n",
    "<ol>\n",
    "  <li>Load the pickle files (<code>cases.pkl</code> and <code>controls.pkl</code>) into memory.</li>\n",
    "  <li>Print the total number of records contained in each file.</li>\n",
    "  <li>Select one random record from each dataset for inspection.</li>\n",
    "  <li>Pretty-print the record using Python’s <code>pprint</code> module for better readability.</li>\n",
    "</ol>\n",
    "\n",
    "<hr/>\n",
    "\n",
    "<h4>Example Output Explanation</h4>\n",
    "\n",
    "<ul>\n",
    "  <li><b>cases.pkl</b>: Contains metadata for patients with cancer cases.\n",
    "    <ul>\n",
    "      <li>Each record includes identifiers for the four standard mammography views:\n",
    "        <ul>\n",
    "          <li><code>L-CC</code>, <code>L-MLO</code> (left breast: craniocaudal and mediolateral oblique)</li>\n",
    "          <li><code>R-CC</code>, <code>R-MLO</code> (right breast: craniocaudal and mediolateral oblique)</li>\n",
    "        </ul>\n",
    "      </li>\n",
    "      <li>Additional fields include:\n",
    "        <ul>\n",
    "          <li><code>best_center</code>: pixel coordinates of the breast’s center region.</li>\n",
    "          <li><code>bottommost_points</code> and <code>rightmost_points</code>: anatomical landmarks used in preprocessing.</li>\n",
    "          <li><code>cancer_label</code>: binary labels (<code>0</code> = no finding, <code>1</code> = malignant).</li>\n",
    "          <li><code>window_location</code>: cropping boundaries for image extraction.</li>\n",
    "          <li><code>horizontal_flip</code>: indicates whether the image should be mirrored.</li>\n",
    "        </ul>\n",
    "      </li>\n",
    "    </ul>\n",
    "  </li>\n",
    "\n",
    "  <li><b>controls.pkl</b>: Contains similar information for healthy control patients, but usually with fewer metadata fields (since no malignant regions are annotated).</li>\n",
    "</ul>\n",
    "\n",
    "<hr/>\n",
    "\n",
    "<h4>Why is this important?</h4>\n",
    "<p>\n",
    "By exploring a few random samples, we can verify:\n",
    "</p>\n",
    "<ul>\n",
    "  <li>That the <code>.pkl</code> files are correctly loaded.</li>\n",
    "  <li>The total number of available patient records.</li>\n",
    "  <li>The structure and type of metadata that will later be used for <b>image preprocessing, alignment, and model inference</b>.</li>\n",
    "</ul>\n",
    "\n",
    "<p>\n",
    "This step ensures that the dataset is properly understood before moving on to <b>image transformations and feeding data into the deep learning pipeline</b>.\n",
    "</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "end2end",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
